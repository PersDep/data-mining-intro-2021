{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "hw05-gd.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "l5cGx9vyBp0x"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f46lxLSsBp0f"
      },
      "source": [
        "# Домашнее задание 5. Градиентный спуск. (10 баллов + 2.5 бонус)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhdyseeXBp0k"
      },
      "source": [
        "В этом домашнем задании вы напишете градиентный спуск для линейной регрессии, а так же посмотрите, как он ведёт себя с разными параметрами и разными функциями потерь."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do1I2ygaBp0k"
      },
      "source": [
        "Правила:\n",
        "\n",
        "* Домашнее задание оценивается в 10 баллов.\n",
        "\n",
        "* Можно использовать без доказательства любые результаты, встречавшиеся на лекциях или семинарах по курсу, если получение этих результатов не является вопросом задания.\n",
        "\n",
        "* Можно использовать любые свободные источники с *обязательным* указанием ссылки на них.\n",
        "\n",
        "* Плагиат не допускается. При обнаружении случаев списывания, 0 за работу выставляется всем участникам нарушения, даже если можно установить, кто у кого списал.\n",
        "\n",
        "* Старайтесь сделать код как можно более оптимальным. В частности, будет штрафоваться использование циклов в тех случаях, когда операцию можно совершить при помощи инструментов библиотек, о которых рассказывалось в курсе.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8Nfs9riBp0k"
      },
      "source": [
        "from typing import List, Iterable\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5OOYVQOBp0l"
      },
      "source": [
        "## Часть 1. Градиентный спуск (5 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j2vGNTyBp0l"
      },
      "source": [
        "Для начала давайте вспомним самый простой функционал ошибки, который мы применяем в задаче регрессии — Mean Squared Error:\n",
        "\n",
        "$$\n",
        "Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell (\\langle x_i, w \\rangle - y_i)^2\n",
        "$$\n",
        "\n",
        "где $x_i$ — это $i$-ый объект датасета, $y_i$ — правильный ответ для $i$-го объекта, а $w$ — веса нашей линейной модели.\n",
        "\n",
        "Как мы помним, для линейной модели, его можно записать в матричном виде вот так:\n",
        "\n",
        "$$\n",
        "Q(w, X, y) = \\frac{1}{\\ell} || Xw - y ||^2\n",
        "$$\n",
        "\n",
        "где $X$ — это матрица объекты-признаки, а $y$ — вектор правильных ответов\n",
        "\n",
        "Для того чтобы воспользоваться методом градиентного спуска, нам нужно посчитать градиент нашего функционала. Для MSE он будет выглядеть так:\n",
        "\n",
        "$$\n",
        "\\nabla_w Q(w, X, y) = \\frac{2}{\\ell} X^T(Xw-y)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05tP14CdBp0l"
      },
      "source": [
        "Ниже приведён базовый класс `BaseLoss`, который мы будем использовать для реализации всех наших лоссов. Менять его не нужно. У него есть два абстрактных метода:\n",
        "1. Метод `calc_loss`, который будет принимать на вход объекты `x`, правильные ответы `y` и веса `w` и вычислять значения лосса\n",
        "2. Метод `calc_grad`, который будет принимать на вход объекты `x`, правильные ответы `y` и веса `w` и вычислять значения градиента (вектор)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWmOOrj6Bp0m"
      },
      "source": [
        "import abc\n",
        "\n",
        "class BaseLoss(abc.ABC):\n",
        "    \"\"\"Базовый класс лосса\"\"\"\n",
        "    @abc.abstractmethod\n",
        "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Функция для вычислений значения лосса\n",
        "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
        "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
        "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
        "        :return: число -- значения функции потерь\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Функция для вычислений градиента лосса по весам w\n",
        "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
        "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
        "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
        "        :return: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
        "        \"\"\"\n",
        "        raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob65L_82Bp0m"
      },
      "source": [
        "Теперь давайте напишем реализацию этого абстрактоного класса: Mean Squared Error лосс."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezASiSCgBp0m"
      },
      "source": [
        "**Задание 1.1 (5/8 балла):** Реализуйте класс `MSELoss`\n",
        "\n",
        "Он должен вычислять лосс и градиент по формулам наверху"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpvu5XryBp0m"
      },
      "source": [
        "class MSELoss(BaseLoss):\n",
        "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Функция для вычислений значения лосса\n",
        "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
        "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
        "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
        "        :return: число -- значения функции потерь\n",
        "        \"\"\"\n",
        "        # -- YOUR CODE HERE --\n",
        "        # Вычислите значение функции потерь при помощи X, y и w и верните его\n",
        "        \n",
        "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Функция для вычислений градиента лосса по весам w\n",
        "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
        "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
        "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
        "        :return: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
        "        \"\"\"\n",
        "        # -- YOUR CODE HERE --\n",
        "        # Вычислите значение вектора градиента при помощи X, y и w и верните его"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "188svk-jBp0n"
      },
      "source": [
        "Теперь мы можем создать объект `MSELoss` и при помощи него вычислять значение нашей функции потерь и градиенты:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoQZnHVRBp0n"
      },
      "source": [
        "# Создадим объект лосса\n",
        "loss = MSELoss()\n",
        "\n",
        "# Создадим какой-то датасет\n",
        "X = np.arange(200).reshape(20, 10)\n",
        "y = np.arange(20)\n",
        "\n",
        "# Создадим какой-то вектор весов\n",
        "w = np.arange(10)\n",
        "\n",
        "# Выведем значение лосса и градиента на этом датасете с этим вектором весов\n",
        "print(loss.calc_loss(X, y, w))\n",
        "print(loss.calc_grad(X, y, w))\n",
        "\n",
        "# Проверка, что методы реализованы правильно\n",
        "assert loss.calc_loss(X, y, w) == 27410283.5, \"Метод calc_loss реализован неверно\"\n",
        "assert np.allclose(loss.calc_grad(X, y, w), np.array([1163180., 1172281., 1181382., 1190483., \n",
        "                                                      1199584., 1208685., 1217786., 1226887., \n",
        "                                                      1235988., 1245089.])), \"Метод calc_grad реализован неверно\"\n",
        "print(\"Всё верно!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqRkUQpABp0n"
      },
      "source": [
        "Теперь когда у нас есть всё для вычисления градиента, давайте напишем наш градиентный спуск. Напомним, что формула для одной итерации градиентного спуска выглядит следующим образом:\n",
        "\n",
        "$$\n",
        "w^t = w^{t-1} - \\eta \\nabla_{w} Q(w^{t-1}, X, y)\n",
        "$$\n",
        "\n",
        "Где $w^t$ — значение вектора весов на $t$-ой итерации, а $\\eta$ — параметр learning rate, отвечающий за размер шага."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9gOWGzqBp0n"
      },
      "source": [
        "**Задание 1.2 (5/8 балла):** Реализуйте функцию `gradient_descent`\n",
        "\n",
        "Функция должна принимать на вход начальное значение весов линейной модели `w_init`, матрицу объектов-признаков `X`, \n",
        "вектор правильных ответов `y`, объект функции потерь `loss`, размер шага `lr` и количество итераций `n_iterations`.\n",
        "\n",
        "Функция должна реализовывать цикл, в котором происходит шаг градиентного спуска (градиенты берутся из `loss` посредством вызова метода `calc_grad`) по формуле выше и возвращать \n",
        "траекторию спуска (список из новых значений весов на каждом шаге)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxu_8RHkBp0o"
      },
      "source": [
        "def gradient_descent(w_init: np.ndarray, X: np.ndarray, y: np.ndarray, \n",
        "                        loss: BaseLoss, lr: float, n_iterations: int = 100000) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Функция градиентного спуска\n",
        "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
        "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
        "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
        "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
        "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
        "    :param n_iterations: int -- сколько итераций делать\n",
        "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
        "    \"\"\"\n",
        "    # -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM-gLeXPBp0o"
      },
      "source": [
        "Теперь создадим синтетический датасет и функцию, которая будет рисовать траекторию градиентного спуска по истории:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgaCBMiDBp0o"
      },
      "source": [
        "# Создаём датасет из двух переменных и реального вектора зависимости w_true\n",
        "\n",
        "np.random.seed(1337)\n",
        "\n",
        "n_features = 2\n",
        "n_objects = 300\n",
        "batch_size = 10\n",
        "num_steps = 43\n",
        "\n",
        "w_true = np.random.normal(size=(n_features, ))\n",
        "\n",
        "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
        "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]  \n",
        "y = X.dot(w_true) + np.random.normal(0, 1, (n_objects))\n",
        "w_init = np.random.uniform(-2, 2, (n_features))\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCsiY4PBBp0o"
      },
      "source": [
        "loss = MSELoss()\n",
        "w_list = gradient_descent(w_init, X, y, loss, 0.01, 100)\n",
        "print(loss.calc_loss(X, y, w_list[0]))\n",
        "print(loss.calc_loss(X, y, w_list[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZm3EqMwBp0p"
      },
      "source": [
        "def plot_gd(w_list: Iterable, X: np.ndarray, y: np.ndarray, loss: BaseLoss):\n",
        "    \"\"\"\n",
        "    Функция для отрисовки траектории градиентного спуска\n",
        "    :param w_list: Список из объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
        "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
        "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
        "    :param loss: Объект подкласса BaseLoss, который умеет считать лосс при помощи loss.calc_loss(X, y, w)\n",
        "    \"\"\"\n",
        "    w_list = np.array(w_list)\n",
        "    meshgrid_space = np.linspace(-2, 2, 100)\n",
        "    A, B = np.meshgrid(meshgrid_space, meshgrid_space)\n",
        "\n",
        "    levels = np.empty_like(A)\n",
        "    for i in range(A.shape[0]):\n",
        "        for j in range(A.shape[1]):\n",
        "            w_tmp = np.array([A[i, j], B[i, j]])\n",
        "            levels[i, j] = loss.calc_loss(X, y, w_tmp)\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.title(\"GD trajectory\")\n",
        "    plt.xlabel(r'$w_1$')\n",
        "    plt.ylabel(r'$w_2$')\n",
        "    plt.xlim(w_list[:, 0].min() - 0.1, \n",
        "             w_list[:, 0].max() + 0.1)\n",
        "    plt.ylim(w_list[:, 1].min() - 0.1,\n",
        "             w_list[:, 1].max() + 0.1)\n",
        "    plt.gca().set_aspect('equal')\n",
        "\n",
        "    # visualize the level set\n",
        "    CS = plt.contour(A, B, levels, levels=np.logspace(0, 1, num=20), cmap=plt.cm.rainbow_r)\n",
        "    CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
        "\n",
        "    # visualize trajectory\n",
        "    plt.scatter(w_list[:, 0], w_list[:, 1])\n",
        "    plt.plot(w_list[:, 0], w_list[:, 1])\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqufHS3PBp0p"
      },
      "source": [
        "**Задание 1.3 (5/8 балла):** При помощи функций `gradient_descent` и  `plot_gd` нарисуйте траекторию градиентного спуска для разных значений длины шага (параметра `lr`). Используйте не менее четырёх разных значений для `lr`. \n",
        "\n",
        "Сделайте и опишите свои выводы о том, как параметр `lr` влияет на поведение градиентного спуска\n",
        "\n",
        "Подсказки:\n",
        "* Функция `gradient_descent` возвращает историю весов, которую нужно подать в функцию `plot_gd`\n",
        "* Хорошие значения для `lr` могут лежать в промежутке от 0.0001 до 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPtpweFLBp0q"
      },
      "source": [
        "# -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdxxpTlSBp0q"
      },
      "source": [
        "Теперь реализуем стохастический градиентный спуск"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNXcI8AqBp0q"
      },
      "source": [
        "**Задание 1.4 (5/8 балла):** Реализуйте функцию `stochastic_gradient_descent`\n",
        "\n",
        "Функция должна принимать все те же параметры, что и функция `gradient_descent`, но ещё параметр `batch_size`, отвечающий за размер батча. \n",
        "\n",
        "Функция должна как и раньше реализовывать цикл, в котором происходит шаг градиентного спуска, но на каждом шаге считать градиент не по всей выборке `X`, а только по случайно выбранной части.\n",
        "\n",
        "Подсказка: для выбора случайной части можно использовать [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) с правильным параметром `size`, чтобы выбрать случайные индексы, а потом проиндексировать получившимся массивом массив `X`:\n",
        "```\n",
        "batch_indices = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
        "batch = X[batch_indices]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNlOVmKLBp0q"
      },
      "source": [
        "def stochastic_gradient_descent(w_init: np.ndarray, X: np.ndarray, y: np.ndarray, \n",
        "                        loss: BaseLoss, lr: float, batch_size: int, n_iterations: int = 1000) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Функция градиентного спуска\n",
        "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
        "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
        "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
        "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
        "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
        "    :param batch_size: int -- размер подвыборки, которую нужно семплировать на каждом шаге\n",
        "    :param n_iterations: int -- сколько итераций делать\n",
        "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
        "    \"\"\"\n",
        "    # -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhnQEuAFBp0r"
      },
      "source": [
        "**Задание 1.5 (5/8 балла):** При помощи функций `stochastic_gradient_descent` и  `plot_gd` нарисуйте траекторию градиентного спуска для разных значений длины шага (параметра `lr`) и размера подвыборки (параметра `batch_size`). Используйте не менее четырёх разных значений для `lr` и `batch_size`. \n",
        "\n",
        "Сделайте и опишите свои выводы о том, как параметры  `lr` и `batch_size` влияют на поведение стохастического градиентного спуска. Как отличается поведение стохастического градиентного спуска от обычного?\n",
        "\n",
        "Обратите внимание, что в нашем датасете всего 300 объектов, так что `batch_size` больше этого числа не будет иметь смысла."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysPrEpkjBp0r"
      },
      "source": [
        "# -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZWZE2VDBp0r"
      },
      "source": [
        "Вы могли заметить, что поведение градиентного спуска, особенно стохастической версии, очень сильно зависит от размера шага. \n",
        "\n",
        "Как правило, в начале спуска мы хотим делать большие шаги, чтобы поскорее подойти поближе к минимуму, а позже мы уже хотим делать шаги маленькие, чтобы более точнее этого минимума достичь и не \"перепрыгнуть\" его. \n",
        "\n",
        "Чтобы достичь такого поведения мы можем постепенно уменьшать длину шага с увеличением номера итерации. Сделать это можно, например, вычисляя на каждой итерации длину шага по следующей формуле:\n",
        "\n",
        "$$\n",
        "    \\eta_t\n",
        "    =\n",
        "    \\lambda\n",
        "    \\left(\n",
        "        \\frac{s_0}{s_0 + t}\n",
        "    \\right)^p\n",
        "$$\n",
        "\n",
        "где $\\eta_t$ — длина шага на итерации $t$, $\\lambda$ — начальная длина шага (параметр `lr` у нас), $s_0$ и $p$ — настраиваемые параметры."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hqj3zJ4Bp0r"
      },
      "source": [
        "**Задание 1.6 (5/8 балла):** Реализуйте функцию `stochastic_gradient_descent` на этот раз с затухающим шагом по формуле выше. Параметр $s_0$ возьмите равным 1. Параметр $p$ возьмите из нового аргумента функции `p`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uL65tOmBp0r"
      },
      "source": [
        "def stochastic_gradient_descent(w_init: np.ndarray, X: np.ndarray, y: np.ndarray, \n",
        "                        loss: BaseLoss, lr: float, batch_size: int, p: float, n_iterations: int = 1000) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Функция градиентного спуска\n",
        "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
        "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
        "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
        "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
        "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
        "    :param batch_size: int -- размер подвыборки, которую нужно семплировать на каждом шаге\n",
        "    :param p: float -- значение степени в формуле затухания длины шага\n",
        "    :param n_iterations: int -- сколько итераций делать\n",
        "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
        "    \"\"\"\n",
        "    # -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwgaqqB5Bp0r"
      },
      "source": [
        "**Задание 1.7 (5/8 балла):** При помощи новой функции `stochastic_gradient_descent` и функции `plot_gd` нарисуйте траекторию градиентного спуска для разных значений параметра `p`. Используйте не менее четырёх разных значений для `p`. Хорошими могут быть значения, лежащие в промежутке от 0.1 до 1.\n",
        "Параметр `lr` возьмите равным 0.01, а параметр `batch_size` равным 10.\n",
        "\n",
        "Сделайте и опишите свои выводы о том, как параметр `p` влияет на поведение стохастического градиентного спуска"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri2bF6iCBp0s"
      },
      "source": [
        "# -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0YKLC4mBp0s"
      },
      "source": [
        "**Задание 1.8 (5/8 балла):** Сравните сходимость обычного градиентного спуска и стохастичекой версии:\n",
        "Нарисуйте график зависимости значения лосса (его можно посчитать при помощи метода `calc_loss`, используя $x$ и $y$ из датасета и $w$ с соответствующей итерации) от номера итерации для траекторий, полученных при помощи обычного и стохастического градиентного спуска с одинаковыми параметрами. Параметр `batch_size` возьмите равным 10.\n",
        "\n",
        "Видно ли на данном графике преимущество SGD? Почему?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_f0qCQsBp0s"
      },
      "source": [
        "# -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7jS7bJUBp0s"
      },
      "source": [
        "## Часть 2. Линейная регрессия (5 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRrNZdJ7Bp0s"
      },
      "source": [
        "Теперь давайте напишем наш класс для линейной регрессии. Он будет использовать интерфейс, знакомый нам из библиотеки `sklearn`.\n",
        "\n",
        "В методе `fit` мы будем подбирать веса `w` при помощи градиентного спуска нашим методом `gradient_descent`\n",
        "\n",
        "В методе `predict` мы будем применять нашу регрессию к датасету, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyKvDkPOBp0s"
      },
      "source": [
        "**Задание 2.1 (5/8 балла):** Допишите код в методах `fit` и `predict` класса `LinearRegression`\n",
        "\n",
        "В методе `fit` вам нужно как-то инициализировать веса `w`, применить `gradient_descent` и сохранить последнюю `w` из траектории.\n",
        "\n",
        "В методе `predict` вам нужно применить линейную регрессию и вернуть вектор ответов.\n",
        "\n",
        "Обратите внимание, что объект лосса передаётся в момент инициализации и хранится в `self.loss`. Его нужно использовать в `fit` для `gradient_descent`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdTk79QTBp0t"
      },
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self, loss: BaseLoss, lr: float = 0.1) -> None:\n",
        "        self.loss = loss\n",
        "        self.lr = lr\n",
        "    \n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "        # Добавляем столбец из единиц для константного признака\n",
        "        X = np.hstack([X, np.ones([X.shape[0], 1])])\n",
        "        \n",
        "        # -- YOUR CODE HERE --\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        # Проверяем, что регрессия обучена, то есть, что был вызван fit и в нём был установлен атрибут self.w\n",
        "        assert hasattr(self, \"w\"), \"Linear regression must be fitted first\"\n",
        "        # Добавляем столбец из единиц для константного признака\n",
        "        X = np.hstack([X, np.ones([X.shape[0], 1])])\n",
        "\n",
        "        # -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV2RGHwgBp0t"
      },
      "source": [
        "Теперь у нас есть наш класс линейной регрессии. Более того, мы можем управлять тем, какую функцию потерь мы оптимизируем, просто передавая разные классы в параметр `loss` при инициализации. \n",
        "\n",
        "Пока у нас нет никаких классов кроме `MSELoss`, но скоро они появятся.\n",
        "\n",
        "Для `MSELoss` мы бы создавали наш объект линейной регрессии, например, так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyfkw-L1Bp0t"
      },
      "source": [
        "linear_regression = LinearRegression(MSELoss())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIG_P49YBp0t"
      },
      "source": [
        "Применим нашу регрессию на реальном датасете. Загрузим датасет с машинами, который был у вас на семинарах:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJn0SUlnBp0t"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "X_raw = pd.read_csv(\n",
        "    \"http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\", \n",
        "    header=None, \n",
        "    na_values=[\"?\"]\n",
        ")\n",
        "X_raw.head()\n",
        "X_raw = X_raw[~X_raw[25].isna()].reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KcER5JtBp0u"
      },
      "source": [
        "y = X_raw[25]\n",
        "X_raw = X_raw.drop(25, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVS4T-IUBp0u"
      },
      "source": [
        "**Задание 2.2 (5/8 балла):** Как обычно обработайте датасет всеми нужными методами, чтобы на нём можно было обучать линейную регрессию:\n",
        "\n",
        "* Разделите датасет на обучающую и тестовую выборку\n",
        "* Заполните пропуски\n",
        "* Нормализуйте числовые признаки\n",
        "* Закодируйте категориальные переменные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COp1ybClBp0u"
      },
      "source": [
        "# -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm7c7zIzBp0u"
      },
      "source": [
        "**Задание 2.3 (5/8 балла):** Обучите написанную вами линейную регрессию на обучающей выборке"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzjAlpliBp0u"
      },
      "source": [
        "# -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr1bF2_bBp0u"
      },
      "source": [
        "**Задание 2.4 (5/8 балла):** Посчитайте ошибку обученной регрессии на обучающей и тестовой выборке при помощи метода `mean_squared_error` из `sklearn.metrics`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2KjQjxvBp0v"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS5Oh4YoBp0v"
      },
      "source": [
        "Наша модель переобучилась. Давайте как обычно в такой ситуации добавим к ней L2 регуляризацию. Для этого нам нужно написать новый класс лосса.\n",
        "\n",
        "Формула функции потерь для MSE с L2 регуляризацией выглядит так:\n",
        "$$\n",
        "Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell (\\langle x_i, w \\rangle - y_i)^2 + \\lambda ||w||^2\n",
        "$$\n",
        "\n",
        "Или в матричном виде:\n",
        "\n",
        "$$\n",
        "Q(w, X, y) = \\frac{1}{\\ell} || Xw - y ||^2 + \\lambda ||w||^2\n",
        "$$\n",
        "\n",
        "Где $\\lambda$ — коэффициент регуляризации\n",
        "\n",
        "Градиент выглядит так:\n",
        "\n",
        "$$\n",
        "\\nabla_w Q(w, X, y) = \\frac{2}{\\ell} X^T(Xw-y) + 2 \\lambda w\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vogzj2W0Bp0v"
      },
      "source": [
        "**Задание 2.5 (5/8 балла):** Реализуйте класс `MSEL2Loss`\n",
        "\n",
        "Он должен вычислять лосс и градиент по формулам наверху"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58FvnEjHBp0v"
      },
      "source": [
        "class MSEL2Loss(BaseLoss):\n",
        "    def __init__(self, coef: float = 1.):\n",
        "        \"\"\"\n",
        "        :param coef: коэффициент регуляризации (лямбда в формуле)\n",
        "        \"\"\"\n",
        "        self.coef = coef\n",
        "    \n",
        "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Функция для вычислений значения лосса\n",
        "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
        "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
        "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
        "        :output: число -- значения функции потерь\n",
        "        \"\"\"\n",
        "        # -- YOUR CODE HERE --\n",
        "        # Вычислите значение функции потерь при помощи X, y и w и верните его\n",
        "        \n",
        "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Функция для вычислений градиента лосса по весам w\n",
        "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
        "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
        "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
        "        :output: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
        "        \"\"\"\n",
        "        # -- YOUR CODE HERE --\n",
        "        # Вычислите значение вектора градиента при помощи X, y и w и верните его"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTl1XQP5Bp0v"
      },
      "source": [
        "Теперь мы можем использовать лосс с l2 регуляризацией в нашей регрессии, например, так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QanspiddBp0v"
      },
      "source": [
        "linear_regression = LinearRegression(MSEL2Loss(0.1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwmC8B0OBp0w"
      },
      "source": [
        "**Задание 2.6 (5/8 балла):** Обучите регрессию с лоссом `MSEL2Loss`. Подберите хороший коэффициент регуляризации и добейтесь улучшения результата на тестовой выборке. Сравните результат на обучающей и тестовой выборке с регрессией без регуляризации. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FId8tKOBp0w"
      },
      "source": [
        "# -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3GSc1JvBp0w"
      },
      "source": [
        "В нашем датасете могут быть выбросы. На семинаре вам рассказывали, что с выбросами хорошо помогает бороться Huber Loss. Вдали от нуля он работает как Mean Absolute Error и не реагирует на выбросы так сильно, как MSE. Давайте его реализуем и применим в нашей регрессии.\n",
        "\n",
        "Напомним, что функция потерь Huber Loss'а  выглядит так:\n",
        "\n",
        "\n",
        "$$\n",
        "    Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell \\phi_\\varepsilon (\\langle x_i, w \\rangle - y_i)\n",
        "$$\n",
        "$$\n",
        "    \\phi_\\varepsilon(z) = \\begin{cases} \\frac 1 2 z^2, - \\varepsilon < z < \\varepsilon, \\\\\\varepsilon (|z| - \\frac 1 2 \\varepsilon), иначе \\\\ \\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "А градиент так:\n",
        "$$\n",
        "    \\nabla_w Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell x_i \\nabla_z \\phi_\\varepsilon (\\langle x_i, w \\rangle - y_i)\n",
        "$$\n",
        "$$\n",
        "    \\nabla_z \\phi_\\varepsilon(z) = \\begin{cases} z, - \\varepsilon < z < \\varepsilon, \\\\\\varepsilon \\text{ sign}(z), иначе \\\\ \\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-y7M1qIBp0w"
      },
      "source": [
        "**Задание 2.7 (5/8 балла):** Реализуйте класс `HuberLoss`\n",
        "\n",
        "Он должен вычислять лосс и градиент по формулам наверху"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqXGo9OLBp0w"
      },
      "source": [
        "class HuberLoss(BaseLoss):\n",
        "    def __init__(self, eps: float) -> None:\n",
        "        \"\"\"\n",
        "        :param eps: параметр huber loss из формулы\n",
        "        \"\"\"\n",
        "        self.eps = eps\n",
        "    \n",
        "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Функция для вычислений значения лосса\n",
        "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
        "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
        "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
        "        :output: число -- значения функции потерь\n",
        "        \"\"\"\n",
        "        # -- YOUR CODE HERE --\n",
        "        # Вычислите значение функции потерь при помощи X, y и w и верните его\n",
        "        \n",
        "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Функция для вычислений градиента лосса по весам w\n",
        "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
        "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
        "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
        "        :output: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
        "        \"\"\"\n",
        "        # -- YOUR CODE HERE --\n",
        "        # Вычислите значение вектора градиента при помощи X, y и w и верните его"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyZkVwLWBp0w"
      },
      "source": [
        "**Задание 2.8 (5/8 балла):** Обучите регрессию с лоссом `HuberLoss`. Сравните результат на обучающей и тестовой выборке с регрессией, обученной c `MSELoss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_BLLnGuBp0w"
      },
      "source": [
        "# -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdIqCkSEBp0w"
      },
      "source": [
        "**Задание 3 (0.08/8 балла)**\n",
        "Вставьте ваш любимый мем 2021 в ячейку ниже:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "numfffXPBp0x"
      },
      "source": [
        "# -- YOUR MEME HERE --"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5cGx9vyBp0x"
      },
      "source": [
        "### БОНУС (2.5 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzpUzkPABp0x"
      },
      "source": [
        "Градиентный спуск — далеко не единственный метод оптимизации. \n",
        "Другой очень известный метод называется [\"Алгоритм имитации отжига\"](https://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_%D0%B8%D0%BC%D0%B8%D1%82%D0%B0%D1%86%D0%B8%D0%B8_%D0%BE%D1%82%D0%B6%D0%B8%D0%B3%D0%B0). Он не так часто используется для оптимизации моделей машинного обучения, но у вас есть уникальная возможность попробовать применить его к нашей любимой линейной регрессии."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Mr6bQLBp0x"
      },
      "source": [
        "**Задание (2.5 баллов)**:\n",
        "Напишите алгоритм имитации отжига для оптимизации MSE линейной регрессии. \n",
        "\n",
        "Сравните результат с градиентным спуском по \"траектории\" и по финальному лоссу.\n",
        "\n",
        "Подсказка: каждую новую точку (веса регресси в нашем случае) можно семплировать из некоторого случайного распределением с центром в текущей точке. Хорошо подойдут распределения с \"тяжёлыми\" хвостами, например, распределение Стьюдента с параметром количества степеней свободы в районе 3.\n",
        "Это может выглядеть, например, так:\n",
        "```\n",
        "new_w = old_w + np.random.standard_t(3, size=old_w.shape)\n",
        "```\n",
        "С параметром распределения можно поэксперементировать: чем он больше, тем реже новые точки будут очень сильно уходить от старых."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xuSuYPzBp0x"
      },
      "source": [
        "# -- YOUR CODE HERE --"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}